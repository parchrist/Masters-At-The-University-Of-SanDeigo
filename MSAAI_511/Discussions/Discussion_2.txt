So when I first start to think about complex issues, I like to often times think about some of the challanges that I will end up facing when building a model or solving a business related issue. When I think about graident decent, I first think about neural networks and image recognition. I find image recognition to be one of the more intresting topics when it comes to neural networks because with hundreds of thousands of pictures, youre essential able to "teach" a ml model how to recognitize patterns or traits in a photo and categorrize them. Another thing that I also think about when building a model, is dealing with some of the issues of data. For example if your data set is very small, you might deal with lots of issues regrading data quality, and have the model be more error prone when moving into prodiction. If I had structured data, I would consider bootstrapping the model to ensure that I could make sure I attempted everything I possibly could before going back to whiteboard. I think that the most common thought I had when I started to train an simple neural works was that the though of exploding or vanishing graident during back propigation. With that being said, I have never ran into the issue myself personally but I do know that it happens and that there are some work arounds to this problem if it does happen. One of the work arounds I remeber reading about a couple years ago was gradient clipping, which is basically setting a threshold value for the graidents (Gradient Clipping | Engati, n.d.) and if the gradients exceed the threshold then they are pulled back so the model does not run into the exploding or vanishing gradient issue. Which from my understanding is that when working with very large models, that it is sometimes smart to put the clipping lines of code in so you dont waste tons of time when working with training and having a bad model.

Citations: 

Gradient clipping | Engati. (n.d.). Engati. https://www.engati.com/glossary/gradient-clipping