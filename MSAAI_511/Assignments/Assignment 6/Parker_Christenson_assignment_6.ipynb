{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parker Christenson Assignment 6\n",
    "\n",
    "### Anomaly Detection with a Deep Autoencoder\n",
    "\n",
    "#### `Instructions`\n",
    "1.  Download the Credit Card Fraud Detection dataset by completing the steps below, and preprocess it as needed for training the deep autoencoder.\n",
    "2.  Go to https://www.kaggle.com/datasets/mlg-ulb/creditcardfraudLinks to an external site. and try to download the dataset.\n",
    "    -   If you don't have access to the dataset, create an account in Kaggle.\n",
    "    -   Once you are logged in, search for \"Credit Card Fraud Detection\" in the search bar at the top of the page.\n",
    "    -   On the dataset page, click the \"Download\" button to download the dataset in CSV format.\n",
    "3.  Split the dataset into training, validation, and testing sets.\n",
    "4.  Design and train a deep autoencoder with multiple layers to encode the input data and decode it back to its original form.\n",
    "5.  Use the trained autoencoder to detect anomalies in the test set by comparing the input and output data and calculating reconstruction error.\n",
    "6.  Evaluate the performance of the autoencoder by measuring the accuracy, precision, recall, and F1 score of the anomaly detection.\n",
    "7.  Discuss the limitations and potential applications of deep autoencoders for anomaly detection in credit card transactions and other domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# autoencoders from pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# sk learns libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
       "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
       "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
       "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
       "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
       "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
       "\n",
       "        V26       V27       V28  Amount  Class  \n",
       "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('creditcard.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\n",
       "       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20',\n",
       "       'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount',\n",
       "       'Class'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and labels\n",
    "features = df.drop(columns=['Class'])\n",
    "labels = df['Class']\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "normalized_features = scaler.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making the testing split\n",
    "X_train, X_test, y_train, y_test = train_test_split(normalized_features, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch dataset\n",
    "class FraudDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = torch.tensor(data, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels.values, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaders and dataset\n",
    "\n",
    "train_dataset = FraudDataset(X_train, y_train)\n",
    "test_dataset = FraudDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model \n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, encoding_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(32, encoding_dim),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(encoding_dim, 32),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(64, input_dim),\n",
    "            nn.Sigmoid() \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting input and encoding dim's and setting model type\n",
    "input_dim = X_train.shape[1]\n",
    "encoding_dim = 16 \n",
    "model = Autoencoder(input_dim, encoding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 0.5072\n",
      "Epoch [2/50], Loss: 0.5503\n",
      "Epoch [3/50], Loss: 1.0595\n",
      "Epoch [4/50], Loss: 0.3465\n",
      "Epoch [5/50], Loss: 0.6035\n",
      "Epoch [6/50], Loss: 0.3398\n",
      "Epoch [7/50], Loss: 0.4453\n",
      "Epoch [8/50], Loss: 0.2655\n",
      "Epoch [9/50], Loss: 0.3725\n",
      "Epoch [10/50], Loss: 0.3139\n",
      "Epoch [11/50], Loss: 0.2322\n",
      "Epoch [12/50], Loss: 0.4286\n",
      "Epoch [13/50], Loss: 0.8521\n",
      "Epoch [14/50], Loss: 0.2665\n",
      "Epoch [15/50], Loss: 0.5925\n",
      "Epoch [16/50], Loss: 0.3181\n",
      "Epoch [17/50], Loss: 0.5288\n",
      "Epoch [18/50], Loss: 0.2621\n",
      "Epoch [19/50], Loss: 0.4673\n",
      "Epoch [20/50], Loss: 0.2739\n",
      "Epoch [21/50], Loss: 1.0693\n",
      "Epoch [22/50], Loss: 0.3406\n",
      "Epoch [23/50], Loss: 0.3143\n",
      "Epoch [24/50], Loss: 0.3272\n",
      "Epoch [25/50], Loss: 0.2748\n",
      "Epoch [26/50], Loss: 0.9863\n",
      "Epoch [27/50], Loss: 0.3094\n",
      "Epoch [28/50], Loss: 0.6825\n",
      "Epoch [29/50], Loss: 0.3556\n",
      "Epoch [30/50], Loss: 0.3520\n",
      "Epoch [31/50], Loss: 0.2767\n",
      "Epoch [32/50], Loss: 0.5378\n",
      "Epoch [33/50], Loss: 0.3191\n",
      "Epoch [34/50], Loss: 1.4361\n",
      "Epoch [35/50], Loss: 0.2752\n",
      "Epoch [36/50], Loss: 0.3678\n",
      "Epoch [37/50], Loss: 0.2717\n",
      "Epoch [38/50], Loss: 0.4093\n",
      "Epoch [39/50], Loss: 1.9841\n",
      "Epoch [40/50], Loss: 0.3230\n",
      "Epoch [41/50], Loss: 0.6607\n",
      "Epoch [42/50], Loss: 1.0113\n",
      "Epoch [43/50], Loss: 0.4028\n",
      "Epoch [44/50], Loss: 0.5676\n",
      "Epoch [45/50], Loss: 0.4551\n",
      "Epoch [46/50], Loss: 0.4222\n",
      "Epoch [47/50], Loss: 0.4757\n",
      "Epoch [48/50], Loss: 2.8322\n",
      "Epoch [49/50], Loss: 0.5660\n",
      "Epoch [50/50], Loss: 0.5368\n"
     ]
    }
   ],
   "source": [
    "# training the model\n",
    "num_epochs = 50 \n",
    "for epoch in range(num_epochs):\n",
    "    for data, _ in train_loader:\n",
    "        output = model(data)\n",
    "        loss = criterion(output, data)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the reconstruction errors\n",
    "def get_reconstruction_errors(model, loader):\n",
    "    model.eval()\n",
    "    errors = []\n",
    "    with torch.no_grad():\n",
    "        for data, _ in loader:\n",
    "            output = model(data)\n",
    "            loss = nn.MSELoss(reduction='none')(output, data).mean(dim=1)\n",
    "            errors.append(loss)\n",
    "    return torch.cat(errors).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and testing errors as vars \n",
    "train_errors = get_reconstruction_errors(model, train_loader)\n",
    "test_errors = get_reconstruction_errors(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting threshold for anomaly\n",
    "threshold = np.percentile(train_errors, 95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomaly detection accuracy: 0.9516\n"
     ]
    }
   ],
   "source": [
    "# calculating the accuracy\n",
    "test_anomalies = test_errors > threshold\n",
    "y_test_numpy = y_test.to_numpy()\n",
    "accuracy = (test_anomalies == y_test_numpy).mean()\n",
    "\n",
    "print(f'Anomaly detection accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9516\n",
      "Precision: 0.0311\n",
      "Recall: 0.8980\n",
      "F1 Score: 0.0600\n"
     ]
    }
   ],
   "source": [
    "# id anomalies in the test set\n",
    "test_anomalies = test_errors > threshold\n",
    "\n",
    "# convert boolean test anomalies to integers (1 for anomaly, 0 for normal)\n",
    "test_anomalies = test_anomalies.astype(int)\n",
    "\n",
    "# labels for the test set\n",
    "y_test_numpy = y_test.to_numpy()\n",
    "\n",
    "# calc the metrics\n",
    "accuracy = accuracy_score(y_test_numpy, test_anomalies)\n",
    "precision = precision_score(y_test_numpy, test_anomalies)\n",
    "recall = recall_score(y_test_numpy, test_anomalies)\n",
    "f1 = f1_score(y_test_numpy, test_anomalies)\n",
    "\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "print(f'Precision: {precision:.4f}')\n",
    "print(f'Recall: {recall:.4f}')\n",
    "print(f'F1 Score: {f1:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
