{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-623ecb49dce2cff2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Classification Using Scikit-learn\n",
    "\n",
    "In this homework you will learn how to build a basic supervised learning algorithm (classification) using the most popular Python machine learning library, scikit-learn. You will follow the 3 canonical steps for building a model:\n",
    "\n",
    "1) Data preparation\n",
    "2) Model fitting\n",
    "3) Model evaluation & selection\n",
    "\n",
    "We will use the World Happiness Report (WHR) data, bringing in some additional information that will enable us to formulate a classification problem to predict categorical labels on the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the code cell below to import some modules and read in and preprocess the WHR data.  The last line in the code cell below returns the head of the basic WHR dataframe, to show you what is in that dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-528dcd796b7a9020",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>year</th>\n",
       "      <th>Happiness</th>\n",
       "      <th>Positive</th>\n",
       "      <th>Negative</th>\n",
       "      <th>LogGDP</th>\n",
       "      <th>Support</th>\n",
       "      <th>Life</th>\n",
       "      <th>Freedom</th>\n",
       "      <th>Generosity</th>\n",
       "      <th>Corruption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2008</td>\n",
       "      <td>3.723590</td>\n",
       "      <td>0.517637</td>\n",
       "      <td>0.258195</td>\n",
       "      <td>7.168690</td>\n",
       "      <td>0.450662</td>\n",
       "      <td>49.209663</td>\n",
       "      <td>0.718114</td>\n",
       "      <td>0.181819</td>\n",
       "      <td>0.881686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2009</td>\n",
       "      <td>4.401778</td>\n",
       "      <td>0.583926</td>\n",
       "      <td>0.237092</td>\n",
       "      <td>7.333790</td>\n",
       "      <td>0.552308</td>\n",
       "      <td>49.624432</td>\n",
       "      <td>0.678896</td>\n",
       "      <td>0.203614</td>\n",
       "      <td>0.850035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2010</td>\n",
       "      <td>4.758381</td>\n",
       "      <td>0.618265</td>\n",
       "      <td>0.275324</td>\n",
       "      <td>7.386629</td>\n",
       "      <td>0.539075</td>\n",
       "      <td>50.008961</td>\n",
       "      <td>0.600127</td>\n",
       "      <td>0.137630</td>\n",
       "      <td>0.706766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2011</td>\n",
       "      <td>3.831719</td>\n",
       "      <td>0.611387</td>\n",
       "      <td>0.267175</td>\n",
       "      <td>7.415019</td>\n",
       "      <td>0.521104</td>\n",
       "      <td>50.367298</td>\n",
       "      <td>0.495901</td>\n",
       "      <td>0.175329</td>\n",
       "      <td>0.731109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2012</td>\n",
       "      <td>3.782938</td>\n",
       "      <td>0.710385</td>\n",
       "      <td>0.267919</td>\n",
       "      <td>7.517126</td>\n",
       "      <td>0.520637</td>\n",
       "      <td>50.709263</td>\n",
       "      <td>0.530935</td>\n",
       "      <td>0.247159</td>\n",
       "      <td>0.775620</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       country  year  Happiness  Positive  Negative    LogGDP   Support  \\\n",
       "0  Afghanistan  2008   3.723590  0.517637  0.258195  7.168690  0.450662   \n",
       "1  Afghanistan  2009   4.401778  0.583926  0.237092  7.333790  0.552308   \n",
       "2  Afghanistan  2010   4.758381  0.618265  0.275324  7.386629  0.539075   \n",
       "3  Afghanistan  2011   3.831719  0.611387  0.267175  7.415019  0.521104   \n",
       "4  Afghanistan  2012   3.782938  0.710385  0.267919  7.517126  0.520637   \n",
       "\n",
       "        Life   Freedom  Generosity  Corruption  \n",
       "0  49.209663  0.718114    0.181819    0.881686  \n",
       "1  49.624432  0.678896    0.203614    0.850035  \n",
       "2  50.008961  0.600127    0.137630    0.706766  \n",
       "3  50.367298  0.495901    0.175329    0.731109  \n",
       "4  50.709263  0.530935    0.247159    0.775620  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import openpyxl\n",
    "%matplotlib inline\n",
    "\n",
    "dfraw = pd.read_excel('WHR2018Chapter2OnlineData.xlsx', sheet_name='Table2.1')\n",
    "\n",
    "\n",
    "cols_to_include = ['country', 'year', 'Life Ladder', \n",
    "                   'Positive affect','Negative affect',\n",
    "                   'Log GDP per capita', 'Social support',\n",
    "                   'Healthy life expectancy at birth', \n",
    "                   'Freedom to make life choices', \n",
    "                   'Generosity', 'Perceptions of corruption']\n",
    "renaming = {'Life Ladder': 'Happiness', \n",
    "            'Log GDP per capita': 'LogGDP', \n",
    "            'Social support': 'Support', \n",
    "            'Healthy life expectancy at birth': 'Life', \n",
    "            'Freedom to make life choices': 'Freedom', \n",
    "            'Perceptions of corruption': 'Corruption', \n",
    "            'Positive affect': 'Positive', \n",
    "            'Negative affect': 'Negative'}\n",
    "df = dfraw[cols_to_include].rename(renaming, axis=1)\n",
    "key_vars = ['Happiness', 'LogGDP', 'Support', 'Life', 'Freedom', 'Generosity', 'Corruption', 'Positive', 'Negative']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-114766d8d149ad20",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Step 1.\n",
    "\n",
    "First, we will augment the core WHR dataset to bring in some additional information that is included in a different worksheet.  Since this is mostly about data processing rather than machine learning, simply execute the next two code cells below.  But study each line of code and the associated comments, and then examine the head of the new dataframe named ```df2``` to understand what has been done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4961999f914bb49e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>South Asia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Albania</td>\n",
       "      <td>Central and Eastern Europe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Algeria</td>\n",
       "      <td>Middle East and North Africa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Angola</td>\n",
       "      <td>Sub-Saharan Africa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Argentina</td>\n",
       "      <td>Latin America and Caribbean</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       country                        region\n",
       "0  Afghanistan                    South Asia\n",
       "1      Albania    Central and Eastern Europe\n",
       "2      Algeria  Middle East and North Africa\n",
       "3       Angola            Sub-Saharan Africa\n",
       "4    Argentina   Latin America and Caribbean"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in data from SupportingFactors worksheet into a new dataframe dfsupp\n",
    "dfsupp = pd.read_excel('WHR2018Chapter2OnlineData.xlsx', sheet_name='SupportingFactors')\n",
    "\n",
    "# extract out region information from SupportingFactors dataframe\n",
    "regions = dfsupp[['country', 'Region indicator']].rename({'Region indicator': 'region'}, axis=1)\n",
    "\n",
    "# examine head of regions dataframe -- each country has an associated world region\n",
    "regions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-04c2e566637e8957",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Happiness</th>\n",
       "      <th>Positive</th>\n",
       "      <th>Negative</th>\n",
       "      <th>LogGDP</th>\n",
       "      <th>Support</th>\n",
       "      <th>Life</th>\n",
       "      <th>Freedom</th>\n",
       "      <th>Generosity</th>\n",
       "      <th>Corruption</th>\n",
       "      <th>region</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>country</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Afghanistan</th>\n",
       "      <td>3.806614</td>\n",
       "      <td>0.580873</td>\n",
       "      <td>0.301283</td>\n",
       "      <td>7.419697</td>\n",
       "      <td>0.517146</td>\n",
       "      <td>50.838271</td>\n",
       "      <td>0.544895</td>\n",
       "      <td>0.118428</td>\n",
       "      <td>0.826794</td>\n",
       "      <td>South Asia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Albania</th>\n",
       "      <td>4.988791</td>\n",
       "      <td>0.642628</td>\n",
       "      <td>0.303256</td>\n",
       "      <td>9.247059</td>\n",
       "      <td>0.723204</td>\n",
       "      <td>68.027213</td>\n",
       "      <td>0.626155</td>\n",
       "      <td>-0.105019</td>\n",
       "      <td>0.859691</td>\n",
       "      <td>Central and Eastern Europe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Algeria</th>\n",
       "      <td>5.555004</td>\n",
       "      <td>0.616524</td>\n",
       "      <td>0.265460</td>\n",
       "      <td>9.501728</td>\n",
       "      <td>0.804633</td>\n",
       "      <td>64.984461</td>\n",
       "      <td>0.536398</td>\n",
       "      <td>-0.208236</td>\n",
       "      <td>0.661478</td>\n",
       "      <td>Middle East and North Africa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Angola</th>\n",
       "      <td>4.420299</td>\n",
       "      <td>0.613339</td>\n",
       "      <td>0.351173</td>\n",
       "      <td>8.713935</td>\n",
       "      <td>0.737973</td>\n",
       "      <td>51.729801</td>\n",
       "      <td>0.455957</td>\n",
       "      <td>-0.077940</td>\n",
       "      <td>0.867018</td>\n",
       "      <td>Sub-Saharan Africa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Argentina</th>\n",
       "      <td>6.406131</td>\n",
       "      <td>0.840998</td>\n",
       "      <td>0.273187</td>\n",
       "      <td>9.826051</td>\n",
       "      <td>0.906080</td>\n",
       "      <td>66.764205</td>\n",
       "      <td>0.753122</td>\n",
       "      <td>-0.154544</td>\n",
       "      <td>0.844038</td>\n",
       "      <td>Latin America and Caribbean</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Happiness  Positive  Negative    LogGDP   Support       Life  \\\n",
       "country                                                                     \n",
       "Afghanistan   3.806614  0.580873  0.301283  7.419697  0.517146  50.838271   \n",
       "Albania       4.988791  0.642628  0.303256  9.247059  0.723204  68.027213   \n",
       "Algeria       5.555004  0.616524  0.265460  9.501728  0.804633  64.984461   \n",
       "Angola        4.420299  0.613339  0.351173  8.713935  0.737973  51.729801   \n",
       "Argentina     6.406131  0.840998  0.273187  9.826051  0.906080  66.764205   \n",
       "\n",
       "              Freedom  Generosity  Corruption                        region  \n",
       "country                                                                      \n",
       "Afghanistan  0.544895    0.118428    0.826794                    South Asia  \n",
       "Albania      0.626155   -0.105019    0.859691    Central and Eastern Europe  \n",
       "Algeria      0.536398   -0.208236    0.661478  Middle East and North Africa  \n",
       "Angola       0.455957   -0.077940    0.867018            Sub-Saharan Africa  \n",
       "Argentina    0.753122   -0.154544    0.844038   Latin America and Caribbean  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute the mean values of all the WHR data for each country, averaging over all years in the dataset\n",
    "dfmean = df.groupby('country').mean().drop('year', axis=1)\n",
    "\n",
    "# merge the mean WHR data with the region information extracted previously\n",
    "df2 = pd.merge(dfmean, regions, on='country').dropna()\n",
    "\n",
    "# set the index of df2 to be the country name\n",
    "df2.set_index('country', inplace=True)\n",
    "\n",
    "# examine head of df2 dataframe -- mean WHR values for each country, along with associated regions\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3ebf9c19dc4374f7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Step 2.\n",
    "\n",
    "This new dataframe ```df2``` is what we want to use for our machine learning task.  For each country in the dataset, we have a set of numerical values ('Happiness', 'Positive', 'Negative', etc., which are all listed in the variable ```key_vars```) and a categorical value ('region').  We would like to know if the raw numerical data are  predictive of the region.  In other words, if someone gave you a set of numerical data on Happiness, etc. for an unknown country, would you be able to predict what region of the world it might be located in?  This is an example of classification, where we will train a model based on the numerical data and the associated labels (regions).\n",
    "\n",
    "In order to proceed, we first want to extract and process some data from our ```df2``` dataframe.  We need to separate the data into two parts:\n",
    "* the region data that we want to be able to predict (we'll call it ```y```)\n",
    "* the WHR numerical data that we want to use as input to our prediction (we'll call it ```x```)\n",
    "\n",
    "Again, our goal is to build a classifier that we will train on a subset of the WHR numerical data (x) and the region data (y), so that we can predict regions from data for countries that we have not trained our model on.\n",
    "\n",
    "In the code cell below:\n",
    "* Extract the subset of ```df2``` associated with the columns in ```key_vars``` and assign it to the variable ```x```.\n",
    "* Extract the subset of ```df2``` associated with the region column, and assign it to the variable ```y```.\n",
    "* Print the shape of both `x` and `y`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graded Cell\n",
    "\n",
    "This cell is worth 5% of the grade for this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-01ec5c8a944da95a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x: (152, 9)\n",
      "Shape of y: (152,)\n"
     ]
    }
   ],
   "source": [
    "# assigning the features to the x and y vars \n",
    "x = df2[key_vars]\n",
    "y = df2['region']\n",
    "\n",
    "# printing the shapes shapes of x and y\n",
    "print(\"Shape of x:\", x.shape)\n",
    "print(\"Shape of y:\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c7c2b22581801780",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Step 3.\n",
    "\n",
    "You should see that the shape of ```x``` is (152, 9) and the shape of ```y``` is (152,).  There are 152 samples (countries), and 9 features (each of the key_vars) that we are using to make predictions.\n",
    "\n",
    "Note that the numerical data columns in ```x``` represent different quantities and have different scales. A key step in machine learning is _standardization_: the transformation of features to be on the same scale (with a mean of 0 and a standard deviation of 1). Standardization can substantially increase model accuracy, performance and interpretability.\n",
    "\n",
    "`sklearn` provides various utilities to perform standardization.  We will use one here called ```StandardScaler```, which will transform a data set so that each resulting column has zero mean and unit standard deviation.\n",
    "\n",
    "Carrying out this scaling is a little complicated if we want to maintain the basic structure of our dataframe, so we have provided the relevant code in the next code cell below.  (The code examples describing StandardScaler in the [sklearn documentation](https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing-scaler) typically just extract out the numerical values in numpy arrays. For this exercise, we'd like to keep the labels together in a dataframe.)\n",
    "\n",
    "Please perform the following steps in the below graded cell:\n",
    "* Import the `StandardScaler` object\n",
    "* Create and fit a `StandardScaler` object to our dataframe ```x```\n",
    "* Create a new dataframe ```x_scaled``` that contains the scaled (transformed) data, using the column and index labels from our unscaled dataframe ```x```\n",
    "* Print out the mean and standard deviation of each column of ```x_scaled```\n",
    "* Peek at the head of the new dataframe ```x_scaled```\n",
    "\n",
    "In examining the output, check that the means of each column have been scaled to nearly zero (to within a very small tolerance) and the standard deviations have been scaled to one. Some of the very small numbers might be printed out in scientific notation, where a number like ```1.928282e-16``` means ```1.928282 * 10**(-16)```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graded Cell\n",
    "\n",
    "This cell is worth 20% of the grade for this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-54c74ff720e1fb98",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Means of each column in x_scaled:\n",
      "Happiness     1.782200e-16\n",
      "LogGDP        6.135443e-17\n",
      "Support      -2.337312e-16\n",
      "Life         -5.843279e-17\n",
      "Freedom       6.748987e-16\n",
      "Generosity    1.168656e-17\n",
      "Corruption    9.349247e-17\n",
      "Positive      1.811417e-16\n",
      "Negative      2.337312e-16\n",
      "dtype: float64\n",
      "\n",
      "Standard deviations of each column in x_scaled:\n",
      "Happiness     1.003306\n",
      "LogGDP        1.003306\n",
      "Support       1.003306\n",
      "Life          1.003306\n",
      "Freedom       1.003306\n",
      "Generosity    1.003306\n",
      "Corruption    1.003306\n",
      "Positive      1.003306\n",
      "Negative      1.003306\n",
      "dtype: float64\n",
      "\n",
      "Head of x_scaled:\n",
      "             Happiness    LogGDP   Support      Life   Freedom  Generosity  \\\n",
      "country                                                                      \n",
      "Afghanistan  -1.443128 -1.438896 -2.425953 -1.333584 -1.397623    0.735439   \n",
      "Albania      -0.360792  0.054466 -0.681799  0.776161 -0.776670   -0.719736   \n",
      "Algeria       0.157600  0.262588  0.007447  0.402698 -1.462554   -1.391919   \n",
      "Angola       -0.881273 -0.381215 -0.556782 -1.224159 -2.077245   -0.543385   \n",
      "Argentina     0.936845  0.527632  0.866136  0.621142  0.193546   -1.042257   \n",
      "\n",
      "             Corruption  Positive  Negative  \n",
      "country                                      \n",
      "Afghanistan    0.451854 -1.262731  0.471370  \n",
      "Albania        0.632648 -0.638194  0.499009  \n",
      "Algeria       -0.456675 -0.902184 -0.030449  \n",
      "Angola         0.672914 -0.934399  1.170248  \n",
      "Argentina      0.546624  1.367958  0.077797  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create and fit the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x)\n",
    "\n",
    "# Transform the data and create a new DataFrame\n",
    "x_scaled = pd.DataFrame(scaler.transform(x), columns=x.columns, index=x.index)\n",
    "\n",
    "# Print mean and standard deviation for each column\n",
    "print(\"Means of each column in x_scaled:\")\n",
    "print(x_scaled.mean())\n",
    "print(\"\\nStandard deviations of each column in x_scaled:\")\n",
    "print(x_scaled.std())\n",
    "\n",
    "# Peek at the head of x_scaled\n",
    "print(\"\\nHead of x_scaled:\")\n",
    "print(x_scaled.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-120c4b66e20c858b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Step 4.\n",
    "\n",
    "Now that the data has been preprocessed, we can begin with our classification analysis.  Let's start by importing some additional tools from `sklearn`.  Execute the code cell below to import:\n",
    "* the ```svm``` and ```tree``` submodules\n",
    "* the ```train_test_split``` function\n",
    "* the ```accuracy_score``` function\n",
    "\n",
    "We'll discuss in more detail below what each of these does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7b61fbfb465b7fba",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import svm, tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9e4b3a68ad0f3755",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Step 5.\n",
    "\n",
    "One of the convenience functions that we imported above is called ```train_test_split```.  As its name suggests, this function splits a dataset into separate training and testing sets.  The [online documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html?highlight=train_test_split#sklearn-model-selection-train-test-split) indicates that it splits a dataset randomly, such that approximately 25% of the data winds up in the test set and the remaining 75% in the training set.  Note that the documentation is a bit confusing, since the function can take a variable number of arrays as inputs. In our case, we want to split up 2 arrays (```x_scaled``` and ```y```) into coordinated test and train sets, so that the function will return a total of 4 subarrays (```x_train, x_test, y_train, y_test```).\n",
    "\n",
    "Because ```train_test_split``` generates random splits of the input data, each time we call the function we will get a different split.  For the purposes of code development, it's useful to be able to get reproducible random numbers or random splits, as it makes debugging and model improvements much easer. This can then be relaxed once one wishes to generate statistics over many random runs.     With ```train_test_split```, this can be accomplished by using the ```random_state``` option; if specified with that state as an integer, then the same random split will be generated each time the function is called (until one changes the value of the integer).  This is known as providing a seed to the pseudo-random number generator that is used by ```train_test_split```.\n",
    "\n",
    "You may enter and execute a call to ```train_test_split``` that takes ```x_scaled``` and ```y``` as inputs, along with the optional parameter ```random_state=0```, and returns the 4 data subsets mentioned above, to be named as ```x_train```, ```x_test```, ```y_train```, ```y_test```.  The online documentation provides an example of what such a function call looks like. After the function call, print the shapes of each of the four arrays that are returned.\n",
    "\n",
    "At first pass, it makes sense to simply apply ```train_test_split()``` directly to ```x_scaled``` and ```y```; however, there is a subtle downside. Performing standardization prior to ```train_test_split()``` potentially leads to 'information leakage' whereby information about the testing dataset (its underlying distribution) is learned during the training phase. This is because the testing data distribution is used to scale the training dataset. \n",
    "\n",
    "In the code cell below, please perform ```train_test_split()``` first before applying ```StandardScaler().fit()``` *only* to the training dataset. Use that fit to transform the training dataset and the testing dataset separately. Ultimately, you should end up with the variables ```x_train_scale```, ```x_test_scale```, ```y_train``` and ```y_test``."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7ba571e171fc1adf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Graded Cell\n",
    "\n",
    "This cell is worth 5% of the grade for this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x_train_scaled: (114, 9)\n",
      "Shape of x_test_scaled: (38, 9)\n",
      "Shape of y_train: (114,)\n",
      "Shape of y_test: (38,)\n"
     ]
    }
   ],
   "source": [
    "# train and test split \n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=0)\n",
    "\n",
    "# creating and fiting the standard scaler to the training split\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x_train)\n",
    "\n",
    "# applying the transforming to both the train and test splits\n",
    "x_train_scaled = scaler.transform(x_train)\n",
    "x_test_scaled = scaler.transform(x_test)\n",
    "\n",
    "# shape printing\n",
    "print(\"Shape of x_train_scaled:\", x_train_scaled.shape)\n",
    "print(\"Shape of x_test_scaled:\", x_test_scaled.shape)\n",
    "print(\"Shape of y_train:\", y_train.shape)\n",
    "print(\"Shape of y_test:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-effebc5e9940ed25",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Step 6.\n",
    "\n",
    "Having split our datasets, we want to first train a classifier on our training data so that we can apply it to the testing data.  One way of assessing the performance of a classifier is to compute its accuracy on the test data. That is, what fraction of the test data are correctly predicted by the classifier?  Fortunately, `sklearn` provides a built-in function named ```accuracy_score``` that carries out this computation. We imported it above, and you can read more about it in the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html?highlight=accuracy_score#sklearn.metrics.accuracy_score).\n",
    "\n",
    "We also imported above the ```svm``` and ```tree``` submodules from sklearn.  These provide support for Support Vector Machine (svm) and Decision Tree (tree) machine learning algorithms.  For more information, review the [Support Vector Machines (SVMs) documentation](https://scikit-learn.org/stable/modules/svm.html) and the [Decision Trees documentation](https://scikit-learn.org/stable/modules/tree.html).  Under the hood, these are very different types of algorithms.  Decision Trees try to formulate a series of yes/no questions based on the data that can distinguish the categories from one another.  SVMs, on the other hand, use techniques from geometry to find cuts through the data space to separate different categories from one another.  Understanding how these methods work in detail is beyond the scope of this exercise, but fortunately (despite the very different data structures and algorithms used internally) `sklearn` provides a uniform interface that lets us easily build these different sorts of classifiers and compare their performance.\n",
    "\n",
    "We will first consider SVMs, and then revisit the problem with Decision Trees.\n",
    "\n",
    "In the code cell below:\n",
    "* create a new ```svm.SVC()``` object and assign it to the variable ```clf1``` &mdash;  a call to ```svm.SVC()``` creates a Support Vector Classifier from the svm submodule, similar to what we did in the earlier exercise on hand-written digits\n",
    "* call the ```fit``` method on ```clf1``` with the `x_train_scale` and `y_train` training data (i.e., training the model to associate ```x_train_scale``` with ```y_train```)\n",
    "* call the ```predict``` method on ```clf1``` on the `x_test_scale` testing data and assign the result to the variable ```predictions1```, in order to make predictions for those inputs\n",
    "* call the ```accuracy_score``` function on the `y` testing data and the test predictions you generated and assign the result to the variable ```score1```\n",
    "* print the value of ```score1```\n",
    "\n",
    "The accuracy score is a fraction between 0 and 1 indicating the fraction of predictions that match the true value in the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graded Cell\n",
    "\n",
    "This cell is worth 20% of the grade for this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-fa487bf06d148c8d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# making the SVM classifier\n",
    "clf1 = svm.SVC()\n",
    "\n",
    "# training the classifier \n",
    "clf1.fit(x_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of SVM classifier: 0.7105263157894737\n"
     ]
    }
   ],
   "source": [
    "# Making predictions on the test data\n",
    "predictions1 = clf1.predict(x_test_scaled)\n",
    "\n",
    "# Calculating the accuracy score\n",
    "score1 = accuracy_score(y_test, predictions1)\n",
    "\n",
    "# Printing the accuracy score\n",
    "print(\"Accuracy of SVM classifier:\", score1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1d88f7095ce3f275",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Step 7.\n",
    "\n",
    "The accuracy score reported should be around 71% (0.71).  This means that approximately 29% of the countries in the test set had their regions mispredicted.  While that doesn't sound great, it could be that the WHR numerical data are not always completely predictive of region. One could imagine some countries that are \"outliers\" in a particular region, and more closely resemble other regions based on the WHR indicators.\n",
    "\n",
    "In the below code cell, please loop over all the predicted and true values in the test set, and prints out the country name and predicted region when the prediction is incorrect.  An output line like: ```Sri Lanka : South Asia -> Sub-Saharan Africa``` means that Sri Lanka is actually part of the South Asia region but was predicted to be part of Sub-Saharan Africa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graded Cell\n",
    "\n",
    "This cell is worth 10% of the grade for this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Israel : Middle East and North Africa -> Western Europe\n",
      "Sri Lanka : South Asia -> Sub-Saharan Africa\n",
      "Tajikistan : Commonwealth of Independent States -> Sub-Saharan Africa\n",
      "Yemen : Middle East and North Africa -> Sub-Saharan Africa\n",
      "Hong Kong S.A.R. of China : East Asia -> Western Europe\n",
      "Philippines : Southeast Asia -> Latin America and Caribbean\n",
      "Italy : Western Europe -> Central and Eastern Europe\n",
      "Slovenia : Central and Eastern Europe -> Western Europe\n",
      "Gabon : Sub-Saharan Africa -> Middle East and North Africa\n",
      "Azerbaijan : Commonwealth of Independent States -> Middle East and North Africa\n",
      "Malaysia : Southeast Asia -> Latin America and Caribbean\n"
     ]
    }
   ],
   "source": [
    "# looping over the test set\n",
    "for idx in x_test.index:\n",
    "    actual_region = y_test.loc[idx]\n",
    "    predicted_region = predictions1[x_test.index.get_loc(idx)]\n",
    "    \n",
    "    # getting the country name \n",
    "    country_name = x_test.loc[idx, 'country'] if 'country' in x_test.columns else idx\n",
    "\n",
    "    # checking and printing \n",
    "    if actual_region != predicted_region:\n",
    "        print(f\"{country_name} : {actual_region} -> {predicted_region}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model evaluation & selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d8cd9531f6db2bc7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Step 8.\n",
    "\n",
    "It is often not obvious what specific algorithm will work best for a particular dataset, so it is good to be able to conduct numerical experiments to see how different methods perform (even if we might not fully understand *why* one method might work better than another).  Because `sklearn` provides a consistent interface to very different types of underlying algorithms, it is easy to build additional classifiers to carry out these kinds of comparisons.  Here, we will build a second classifier based on Decision Trees as supported by the ```tree``` module.  Decision Tree algorithms have an element of randomness to them, so a Decision Tree can also be constructed with a specified ```random_state```  such as an integer that seeds the random number generator.  Most of what we will do here is very similar to the code you wrote a few cells up when you built a SVC classifier.\n",
    "\n",
    "In the code cell below:\n",
    "\n",
    "* Create a new ```tree.DecisionTreeClassifier()``` object with the optional argument ```random_state=0```, and assign it to the variable ```clf2``` (`clf2` stands for \"classifier number 2\", so that we can compare with ```clf1``` above).\n",
    "* Call the ```fit``` method on ```clf2``` with the `x_train_scale` and `y_train` training data (i.e., training the model to associate ```x_train_scale``` with ```y_train```).\n",
    "* Call the ```predict``` method on ```clf2``` on the `x_test_scale` testing data and assign the result to the variable ```predictions2```, in order to make predictions for those inputs.\n",
    "* Call the ```accuracy_score``` function on the `y_test` testing data and the test predictions you generated and assign the result to the variable ```score2```.\n",
    "* Print the value of ```score2```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graded Cell\n",
    "\n",
    "This cell is worth 10% of the grade for this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-da49b9de6d360166",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Decision Tree classifier: 0.7631578947368421\n"
     ]
    }
   ],
   "source": [
    "# creating the decision tree classifier\n",
    "clf2 = tree.DecisionTreeClassifier(random_state=0)\n",
    "\n",
    "# training the classifier\n",
    "clf2.fit(x_train_scaled, y_train)\n",
    "\n",
    "# predicitions on the test \n",
    "predictions2 = clf2.predict(x_test_scaled)\n",
    "\n",
    "# calculating the accuracy score\n",
    "score2 = accuracy_score(y_test, predictions2)\n",
    "\n",
    "# accuracy score\n",
    "print(\"Accuracy of Decision Tree classifier:\", score2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0d4bfe43911002ff",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Step 9.\n",
    "\n",
    "We ran two classifiers &mdash; ```clf1``` (SVM) and ```clf2``` (Decision Tree) &mdash; on a particular random `train_test_split` of the full dataset.  We can't really reach any conclusions about the relative performance of the two methods just by considering one split.  Given that ```train_test_split``` can produce different random splits, let's write a little code to compare the two classifiers for different splits.\n",
    "\n",
    "In the code cell below, write some code to do the following:\n",
    "* Write a Python `for` loop so that you can run through the loop 20 times\n",
    "* Within each pass through the loop, do the following:\n",
    "    * Call `test_train_split` on ```x``` and ```y``` to get new random instances of `x_train`, `x_test`, `y_train`, `y_test` -- in this case, you don't want to pass in a value for ```random_state``` since you want to get different random splits each time\n",
    "    * Fit StandardScaler to `x_train`, and use it to transform both `x_train` and `x_test` into `x_train_scaled` and `x_train_test`\n",
    "    * Fit each of the classifiers `clf1` and `clf2` to `x_train_scaled` and `y_train`\n",
    "    * Run predictions on each of the classifiers `clf1` and `clf2` on the `x_test_scaled` and `y_test` testing data\n",
    "    * Compute the accuracy_score of each of the two classifiers on the test data and the test predictions you generated \n",
    "    * Print the score of each classifier, as well as their difference (hint: ```print(score1, score2, score1-score2)``` to get just one line of output per iteration of the loop)\n",
    "    \n",
    "Execute the code you have written.  You should see it run through the loop 20 times, for different random data splits.  While the overall performance varies from run to run, you should probably see that the SVC classifier (```clf1```) generally performs a little bit better than the DecisionTree classifier (```clf2```).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graded Cell\n",
    "\n",
    "This cell is worth 10% of the grade for this assignment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-4b11d4e6c2398273",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7105263157894737 0.5789473684210527 0.13157894736842102\n",
      "0.6578947368421053 0.6052631578947368 0.052631578947368474\n",
      "0.631578947368421 0.6052631578947368 0.02631578947368418\n",
      "0.5526315789473685 0.5 0.052631578947368474\n",
      "0.7105263157894737 0.5526315789473685 0.1578947368421052\n",
      "0.6578947368421053 0.5 0.1578947368421053\n",
      "0.7105263157894737 0.5526315789473685 0.1578947368421052\n",
      "0.7368421052631579 0.6842105263157895 0.05263157894736836\n",
      "0.7631578947368421 0.7368421052631579 0.026315789473684292\n",
      "0.5526315789473685 0.5526315789473685 0.0\n",
      "0.7894736842105263 0.7631578947368421 0.02631578947368418\n",
      "0.5526315789473685 0.5789473684210527 -0.02631578947368418\n",
      "0.6842105263157895 0.7631578947368421 -0.07894736842105265\n",
      "0.631578947368421 0.5 0.13157894736842102\n",
      "0.7105263157894737 0.5789473684210527 0.13157894736842102\n",
      "0.6842105263157895 0.6052631578947368 0.07894736842105265\n",
      "0.7105263157894737 0.6578947368421053 0.05263157894736836\n",
      "0.7631578947368421 0.6578947368421053 0.10526315789473684\n",
      "0.8157894736842105 0.7105263157894737 0.10526315789473684\n",
      "0.7368421052631579 0.5526315789473685 0.18421052631578938\n"
     ]
    }
   ],
   "source": [
    "# getting the two classifiers\n",
    "clf1 = svm.SVC()\n",
    "clf2 = tree.DecisionTreeClassifier()\n",
    "\n",
    "# for loop running random splits \n",
    "for i in range(20):\n",
    "    # splitting\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y)\n",
    "\n",
    "    # scaling\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(x_train)\n",
    "    x_train_scaled = scaler.transform(x_train)\n",
    "    x_test_scaled = scaler.transform(x_test)\n",
    "\n",
    "    # fitting\n",
    "    clf1.fit(x_train_scaled, y_train)\n",
    "    clf2.fit(x_train_scaled, y_train)\n",
    "\n",
    "    # predicitions\n",
    "    predictions1 = clf1.predict(x_test_scaled)\n",
    "    predictions2 = clf2.predict(x_test_scaled)\n",
    "\n",
    "    # getting accuracy scores\n",
    "    score1 = accuracy_score(y_test, predictions1)\n",
    "    score2 = accuracy_score(y_test, predictions2)\n",
    "\n",
    "    # prinintg scores \n",
    "    print(score1, score2, score1 - score2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-01de5a27c9f1842e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Step 10.\n",
    "\n",
    "In the last code cell, you printed out the scores of the two classifiers for a small number of random splits, and examined the numerical output.  Perhaps you'd rather generate a visual summary of the relative performance of the two classifiers, for a larger number of runs.\n",
    "\n",
    "In the code cell below, copy and paste the code you wrote above and modify it to do the following:\n",
    "\n",
    "* prior to entering the `for` loop, initialize two empty lists named ```all_scores1``` and ```all_scores2``` that will be used to collect the scores of each classifier each time through the loop\n",
    "* run through the loop 1000 times instead of 20 as before\n",
    "* append the scores (```score1``` and ```score2```) to each of the lists used to contain all the scores\n",
    "* remove the print statement so that you don't get 1000 annoying print statements when you run the code\n",
    "* once the loop is finished, use the ```plt.hist``` function to plot histograms for ```all_scores1``` and ```all_scores2``` together in the same plot\n",
    "    * you can accomplish this by making two successive calls to the histogram function within the same code cell\n",
    "    * you might want to add options to change the number of bins for the histograms\n",
    "    * you should change the alpha value (opacity) of the histogram plots so that you can see both distributions, since at full opacity, the second one plotted will obscure the first one\n",
    "    * you should use the ``label`` option to label the datasets\n",
    "* After making your two calls to ```plt.hist```, you should call ``plt.legend`` to produce a legend on the plot that will identify the two datasets based on the label options that you added to your ```plt.hist``` calls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graded Cell\n",
    "\n",
    "This cell is worth 20% of the grade for this assignment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-28a13e824292104e",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# initializing the classifiers\n",
    "clf1 = svm.SVC()\n",
    "clf2 = tree.DecisionTreeClassifier()\n",
    "\n",
    "# lists to collect scores\n",
    "all_scores1 = []\n",
    "all_scores2 = []\n",
    "\n",
    "# for loop to run through 1000 random splits\n",
    "for _ in range(1000):\n",
    "    # splitting the sets \n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y)\n",
    "\n",
    "    # scaling the data\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(x_train)\n",
    "    x_train_scaled = scaler.transform(x_train)\n",
    "    x_test_scaled = scaler.transform(x_test)\n",
    "\n",
    "    # fitting the classifiers\n",
    "    clf1.fit(x_train_scaled, y_train)\n",
    "    clf2.fit(x_train_scaled, y_train)\n",
    "\n",
    "    # predictions\n",
    "    predictions1 = clf1.predict(x_test_scaled)\n",
    "    predictions2 = clf2.predict(x_test_scaled)\n",
    "\n",
    "    # collecting the scores\n",
    "    score1 = accuracy_score(y_test, predictions1)\n",
    "    score2 = accuracy_score(y_test, predictions2)\n",
    "    all_scores1.append(score1)\n",
    "    all_scores2.append(score2)\n",
    "\n",
    "# making histograms of the scores\n",
    "plt.hist(all_scores1, bins=30, alpha=0.7, label='SVM Scores')\n",
    "plt.hist(all_scores2, bins=30, alpha=0.7, label='Decision Tree Scores')\n",
    "plt.legend()\n",
    "plt.title(\"Comparative Accuracy Scores of SVM and Decision Tree Classifiers\")\n",
    "plt.xlabel(\"Accuracy Score\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5096838c0f9fe754",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Just scratching the surface...\n",
    "\n",
    "This is just the start of what you can do with scikit-learn.  It is clear from the documentation that there are many different methods and algorithms for classification that are supported by the package, as well as different ways of optimizing and assessing the performance of different algorithms.  If you are motivated to explore further, feel free to continue below by opening more code cells and using the scikit-learn documentation to guide some further exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What to Submit?\n",
    "\n",
    "Please run your Jupyter Notebook first to generate outputs for each code cell and then export the report as a HTML file by clicking the following links (File -> Download as -> HTML (.html)). Please zip both the Jupyter Notebook and the HTML file and submit your ZIP file."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
